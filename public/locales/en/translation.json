{
  "what": {
    "title": "What is this?",
    "p1": "An interactive web visualiser for a Q-learning RL agent that plays Snake.",
    "p2": "Set your own hyperparameters and see how the algorithm performs.",
    "p3": "Uses"
  },
  "how": {
    "title": "How do I use this?",
    "p1": "Just set the parameters below and hit Train.",
    "p2": "Vary the speed and click Test to see how the AI plays after training.",
    "p3": "Explanation of each parameter and the code for this project is available on"
  },
  "parameters": {
    "title": "Parameters",
    "start": "Start Epsilon",
    "discount": "Discount Factor",
    "end": "End Epsilon",
    "episodes": "Episodes",
    "train": "Train",
    "stops": "Stop",
    "test": "Test",
    "note": "Note: Train button resets Q-table"
  },
  "speed": {
    "title": "Speed Control",
    "delay": "Delay between moves:"
  },
  "currentRun": {
    "title": "Current Run",
    "currentEpsilon": "Current Epsilon",
    "currentScore": "Current Score",
    "maxScore": "Max Score"
  },
  "agentSee": {
    "title": "What does the agent see?",
    "p1": " Hover over the boxes present below for the exact details of the state representation."
  },
  "state": {
    "tooltip1": "The presence of immediate danger (either a wall or the snake body itself) one step ahead in all four directions. There could be danger in multiple directions at the same time, so 4 directions -> 16 possible combinations.",
    "tooltip2": "The relative position of the apple with respect to the snake. This has 7 possible values: Bottom Left, Left, Top Left, Up and so on."
  },
  "qtable": {
    "title": "Q Table",
    "p1": "The Q-table shown above has dimensions 8 x 16 (with 4 entries in each cell for each move, here we just show the best move learnt so far).",
    "p2": "Each cell in the grid is a state, ie: one situation the snake finds itself in, like the apple is in the top left direction and there is danger to left, which move do I make - up, left, down, or right?",
    "p3": "The blank entries correspond to unexplored states. So initially, all states are unexplored. As the AI plays the game, it explores the different states and tries to learn what moves work (based on the reward for each action made).",
    "p4": "The red entries correspond to explored states with wrong move learnt by the AI.",
    "p5": "The green entries correspond to explored states with right move learnt by the AI (ie: what move a human would make).",
    "p6": "The 8 rows correspond to: Relative location of the apple to the head (8 directions)",
    "p7": "The 16 columns correspond to: Presence of danger one step ahead of the head in 4 directions (array of 4 numbers, which results in 16 possible values)."
  }
}
